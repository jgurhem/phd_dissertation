\chapter{Taxonomy of Task-Based Programming Models}
\label{chap:taxonomy}

In this chapter, the taxonomy and the properties deduced from the usage of several task based programming models is introduced.
This taxonomy also presents how each property is expressed in the different tasks programming models.

\section{Taxonomy}
The taxonomy of the task based programming models introduced in the previous chapter is detailed in this section.

\subsection{Task Capabilities}

\subsubsection{Task Granularity}
This property represents the amount of resources on which a task can be executed.
For this study, we consider that either \textit{sequential}, \textit{parallel} or \textit{parallel and distributed} code can be executed as task.
Usually, a sequential task is run as a lightweight thread, a thread or a single process without multi-threading.
A parallel task can also use multiple threads or processes up to one node like OpenMP or MPI on shared memory.
Finally, distributed and parallel resources like multiple nodes from a cluster can be allocated to parallel and distributed tasks.
These tasks could be able to execute MPI or PGAS based code on their allocated resources.

As seen in Chapter \ref{chap:exp_dense}, a good balance between the number of tasks and the work load given to the task is required to express enough parallelism so that the computing resources can execute tasks and to make sure there is not too much overhead from having too many tasks.
Therefore, sequential tasks are more likely to be fine grain tasks that process a relatively small amount of data while distributed and parallel tasks are more likely to process more data and have a larger grain.
For instance, YML+XMP tasks are distributed and parallel while HPX, PaRSEC and Regent tasks are sequential tasks that run in threads.
In Table \ref{tab:blocks}, we explore the number of blocks used in YML+XMP, HPX, PaRSEC and Regent to get the best results for our task based LU factorization for a 16384 $\times$ 16384 matrix.
The size of the block can be computed by dividing the size of the matrix by the number of blocks.
Therefore, the higher the number of blocks, the smaller the size of the block is.
In this case, there is up to 8 $\times$ 8 blocks for YML+XMP whereas there is at least 30 $\times$ 30 blocks for Regent, HPX and PaRSEC.
Thus, YML+XMP distributed and parallel tasks managed around 16 times ($ \sim (30/8)^2 $) more data than the largest task executed by Regent, HPX and PaRSEC.
Moreover, these tasks are matrix products and matrix inversions which correspond to $\sim n^3$ operations where $n$ is the dimension of the matrix.
So, YML+XMP tasks performed around 64 times ($ \sim (30/8)^3 $) more operations.

In Table \ref{tab:taxo:prop_task_granularity}, the granularity of the tasks of the considered task based programming models is presented.
Most of the task based programming models use sequential tasks.
For our studies and experiments, we used task based programming models for distributed memory with sequential tasks and with parallel and distributed tasks.

\begin{table}[H]
	\caption{Task Granularity property for each task based programming model \label{tab:taxo:prop_task_granularity}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Task Granularity_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Task Granularity_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Architecture}
The architecture supported by the task based programming models is an important property to take into consideration since it will impact the scale at which the task based programming models can be used.
There is two main scale used in high performance computing : the shared memory and the distributed memory.
Shared memory involves the usage of limited resources since these task based programming models can only be used on one cluster or supercomputer since they do not provide means to transfer data between several nodes.
They can be used with data transfer libraries to address distributed memory.
One of such case is the use of OpenMP on the nodes and MPI to manage distributed memory.
They can also be used in a task that could run on a complete node.
On the other hand, with distributed memory, it is possible to address multiple nodes, thus, as much resources as available.
In this case, the programming models that can manage distributed memory are able to migrate data between cluster nodes.
Therefore, the two values considered for this property are the \textit{shared memory} and the \textit{distributed memory} architectures.

Shared memory task based programming models cannot be used to address exascale computing without being used with another technology to manage distributed memory.

In Table \ref{tab:taxo:prop_architecture}, the architecture which is supported by the considered task based programming models is introduced.
Task based programming models that manage shared memory can only execute sequential tasks expect for QUARK that can execute multi-threaded functions.
On the other hand, some of the task based programming models that manage distributed can execute parallel and distributed tasks.

\begin{table}[H]
	\caption{Architecture property for each task based programming model \label{tab:taxo:prop_architecture}}
	\centering
	\begin{multicols}{2}
		\input{chapters/taxonomy/_table_taxo_Architecture_p1.tex}

		\input{chapters/taxonomy/_table_taxo_Architecture_p2.tex}
	\end{multicols}
\end{table}

\subsubsection{Heterogeneity}
This property indicates if the programming model supports accelerators (for instance, GPUs).
\textit{Explicit} support means that the user has to provide the implementation of the task that will be run on the accelerator.
\textit{Implicit} support means that the tasks can be run on different devices while the user has to provide only one implementation.
In this case, the programming models also manage data transfer between the CPUs and the GPUs memory.
For instance, tasks implemented with OpenMP can be run both on CPUs and GPUs while OpenMP manages the data offloads to the GPU.

In Table \ref{tab:taxo:prop_heterogeneity}, it is shown if the accelerator support is explicit or implicit for the considered task based programming models.
Most of the task based programming models do not support a direct implementation of the tasks for accelerators.
Indeed, the users have to provide such implementation if they want to execute tasks addressing accelerator computations.
Our experiments were mostly performed on CPUs so this feature was not used during our experiments.

\begin{table}[H]
	\caption{Heterogeneity property for each task based programming model \label{tab:taxo:prop_heterogeneity}}
	\centering
	\begin{multicols}{2}
		\input{chapters/taxonomy/_table_taxo_Heterogeneity_p1.tex}

		\input{chapters/taxonomy/_table_taxo_Heterogeneity_p2.tex}
	\end{multicols}
\end{table}

\subsubsection{Data Handling}
This property describes how the data are accessed in a task.
The data can be accessed \textit{directly}.
For instance, the data can be accessed through function parameters.
The data can also be requested or retrieved from a \textit{container}.
For instance, the data can be retrieved through a future in HPX.

This represents the current (as experienced during this dissertation) implementations of the task based programming models and can evolve with time.

In Table \ref{tab:taxo:prop_data_handling}, how to access the data from the tasks is described.
Most of the programming models provide a direct access to the input and output data for tasks whereas only a few of them require the user to request for the data they use in their tasks.
During our experiments, we used HPX that uses futures to retrieve data in tasks and YML+XMP that provide a direct access to the data.

\begin{table}[H]
	\caption{Data Handling property for each task based programming model \label{tab:taxo:prop_data_handling}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Data Handling_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Data Handling_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Task Implementation}
This property indicates what kind of interface the user has to fill in to create a task which will be executed during the execution of the application.
The tasks can use a \textit{program} where the user has to provide the parameters through the API such as Pegasus.
It can also use a \textit{function pointer} which the user has to pass to the API as well as its parameters.
The programing paradigm can also be based on \textit{pragmas} which are used to delimit and describe the task.
Another possibility is a custom interface like a \textit{function with specific parameters}.
The tasks can also be implemented by \textit{encapsulating} them such as in YML+XMP where the task code is encapsulated in an XML file.
Moreover, the tasks can be implemented using the dedicated language of the task based programming model.
For instance, Regent task have to be implemented with Regent which allows the conversion of the tasks into CPU and GPU code.

Task encapsulation or function calls with existing softwares is an important feature and useful for task based programming models since it allows to reuse already implemented functions in tasks.
The programming model should be able to provide a way to easily access data from a task to call an existing library e.g. existing code already written in C/C++/Fortran.
This allows faster developments since existing code can be reused.

In Table \ref{tab:taxo:prop_task_implementation}, how the tasks are implemented is detailed.
During our experiments, we used Regent with its dedicated language based on Lua and Terra to implement applications and tasks.
We also used HPX that uses function pointers to implement tasks as well as YML+XMP and PaRSEC.

\begin{table}[H]
	\caption{Task Implementation property for each task based programming model \label{tab:taxo:prop_task_implementation}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Task Implementation_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Task Implementation_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Portability Accelerators}
This property indicates what kind of accelerators are supported by the task based programming models.
They can support or generate \textit{CUDA} code and execute it on NVIDIA GPUs.
There is also the possibility of supporting \textit{multiple} accelerator architectures.
This may be achieved by having a suitable backend for each supported accelerator architecture.
Another possibility is to let the \textit{user} chose how and which type of accelerator to use by interfacing with an accelerator programming language.

It depends on the existing technologies and can evolve over time or with apparition of new technologies.

Furthermore, the multiplication of supercomputer architectures makes the portability between supercomputers difficult especially if the support of multiple architectures is necessary.
Therefore, the portability of an application is mandatory if there is a large amount of users.
Indeed, in a large user base, there is a high probability that the users will have access to different computer architectures.
At least, the architecture of their local computers should be different than the supercomputer they have access to.
Portability to a large set of architectures allows the users to be able to familiarize with the application locally and the developers to easily make local modifications and tests then experiment them on larger scale on supercomputers.

In Table \ref{tab:taxo:prop_portability_accelerators}, the accelerators supported by the different task based programming models are shown.
Our applications do not support accelerators so we were not able to try this feature.
Nvidia cards are the most used GPUs so CUDA that can address Nvidia GPU programming is supported by most of the task programming models that can execute tasks on GPUs.
However, other constructors are producing GPUs such as Intel and AMD.

\begin{table}[H]
	\caption{Portability Accelerators property for each task based programming model \label{tab:taxo:prop_portability_accelerators}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Portability Accelerators_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Portability Accelerators_p2.tex"}
	\end{multicols}
\end{table}

\subsection{Task and Data Management}
\subsubsection{Dependency Type}
This property describes how the dependencies between the tasks are provided by the user of the task based programming models.
This means that depending on the provided informations (in task description, in particular), the scheduler can deduce other dependency types.
For instance, the control dependency graph combined with the informations on the use of the data parameters provided to the task could allow the scheduler to deduce the data dependency graph.
Therefore, the user has to provide the required informations so that the scheduler is able to work properly.

The user may have to provide \textit{control} dependencies between the tasks.
They describe in which order the tasks can be executed and which tasks can be executed at the time.
\textit{Data} dependencies can also be provided.
This approach is used to infer the dependencies and the parallelism between the tasks by studying how the data are used by the tasks and how they flow from one task to the other.
The user may have to provide \textit{both} dependency type although it is possible to convert a dependency graph type into the other with the appropriate informations.

In Table \ref{tab:taxo:prop_dependency_type}, the dependency type used to describe the dependencies between the tasks in task based programming models is introduced.
In our experiments, we used data oriented such as HPX and control oriented dependency programming models such as YML+XMP.

\begin{table}[H]
	\caption{Dependency Type property for each task based programming model \label{tab:taxo:prop_dependency_type}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Dependency Type_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Dependency Type_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Worker Management}
This property indicates whether the worker thread or process which hosts the tasks in the task based programming models has to be started and maintained by the user (\textit{explicit}) or is provided by the runtime (\textit{implicit}).
This means the user has to put more efforts to launch its application in the explicit case.

In Table \ref{tab:taxo:prop_worker_management}, the worker management is shown for each task based programming model.
Most of the task based programming models manage their worker placement to execute the tasks without user interference.
Only a few need user intervention while some of them allow hints from the user.

\begin{table}[H]
	\caption{Worker Management property for each task based programming model \label{tab:taxo:prop_worker_management}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Worker Management_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Worker Management_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Data Distribution}
This property describes how the data distribution is handled by the programming model scheduler.
The scheduler can be able to migrate data between computing resources.
\textit{Implicit} data distribution means that the runtime system decides where to place the data on the nodes whereas \textit{explicit} data distribution means that the user has to specify the distribution of the data across the nodes.
In the implicit case, the scheduler has more liberty to optimize the computations and the data migrations when the scheduler is able to do so.

When the schedulers will be more efficient and perform better, the implicit management of the data will be very interesting to manage data.
However, it is still not the case as we showed in Chapter \ref{chap:exp_km}.

In Table \ref{tab:taxo:prop_data_distribution}, the data distribution management in the task based programming models is introduced.
In our YML+XMP applications, we had implicit data distributions for the tasks but not in the parallel and distributed tasks in which we had to place our data.
For our HPX and PaRSEC applications, we had to explicitly map our data with process ranks representing cores or nodes from a cluster or supercomputer.

\begin{table}[H]
	\caption{Data Distribution property for each task based programming model \label{tab:taxo:prop_data_distribution}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Data Distribution_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Data Distribution_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Task Binding}
This property describes how the tasks are bound to the allocated hardware resources.
It demands more effort to the scheduler in order to place the tasks on the resources but with efficient enough schedulers, it will allow to increase the performances of the task based programming models.
The binding can be \textit{implicit} when automatically determined by the task based programming runtime or \textit{explicit} when the user has to provide a binding map.

In Table \ref{tab:taxo:prop_task_binding}, the task binding management of the task based programming models is highlighted.
All the programming models propose a implicit task binding with some of them allowing the user to pin tasks on computing resources.
For our experiments we did not try to optimize the task binding by giving hints to PaRSEC and HPX that support explicit bindings.

\begin{table}[H]
	\caption{Task Binding property for each task based programming model \label{tab:taxo:prop_task_binding}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Task Binding_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Task Binding_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Task Insertion}
This property indicates if new tasks can be added to the task pool during the execution of the already scheduled tasks.
This feature was not required for the studied applications but it is an important feature for certain applications such as iterative methods.
This feature can allow them to start a new iteration or stop iterating according to a given metrics that can be the output of a task.
For instance, HPX provides such features.

In Table \ref{tab:taxo:prop_task_insertion}, the insertion of new tasks during the execution of the tasks is described.
This feature is very useful for iterative methods but we did not implement such methods so we did not use this feature in our experiments.

\begin{table}[H]
	\caption{Task Insertion property for each task based programming model \label{tab:taxo:prop_task_insertion}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Task Insertion_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Task Insertion_p2.tex"}
	\end{multicols}
\end{table}

\subsection{Programming Model Features}
\subsubsection{Dependency Expression}
This property describes how the dependencies between the tasks are represented in the considered model.
The possibilities include a \textit{graph}, a \textit{directed acyclic graph} or DAG, a \textit{tree} and a \textit{Petri Net}.

In Table \ref{tab:taxo:prop_dependency_expression}, how the dependency are expressed in each task based programming model is detailed.
The difference between graph and DAG is not always clear in the implementations of the task based programming models.
In this table, we recorded the informations we found in papers introducing the task based programming models.

\begin{table}[H]
	\caption{Dependency Expression property for each task based programming model \label{tab:taxo:prop_dependency_expression}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Dependency Expression_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Dependency Expression_p2.tex"}
	\end{multicols}
\end{table}


\subsubsection{Communication Model}
This property describes how data are sent from a task to another.
The runtime system can use \textit{message passing} (msg), \textit{global address space} (gas) or the \textit{file system} (fs).

This feature can provide many interesting optimizations such as data migration anticipation or data pre-loading.
However, these features are not implemented yet.
They also induces complex choices for the scheduler that may not improve performances at the end.

In Table \ref{tab:taxo:prop_communication_model}, the communication model used in each task based programming model is shown.
Task based programming models that only work in shared memory do not have a communication model since they do not manage distributed memory.
In our applications, we used programming models that use each communication model.

\begin{table}[H]
	\caption{Communication Model property for each task based programming model \label{tab:taxo:prop_communication_model}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Communication Model_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Communication Model_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Fault Tolerance}
This property indicates if the task based programming models support fault tolerance.
Fault tolerance allows applications to recover from errors during execution.
If errors still appear, the application can be stopped properly.

In Table \ref{tab:taxo:prop_fault_tolerance}, the fault tolerance support is given for each task based programming model.
Task based programming models that support fault tolerance are mainly the ones that can execute distributed and parallel tasks such as YML+XMP we used to implement our applications.

\begin{table}[H]
	\caption{Fault Tolerance property for each task based programming model \label{tab:taxo:prop_fault_tolerance}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Fault Tolerance_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Fault Tolerance_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Implementation Type}
This property describes how the programming model API is included in an application.
It can be done through a \textit{library}, a \textit{language extension} or a \textit{language}.
This property shows how the programming models are used to implement an application with them.

In Table \ref{tab:taxo:prop_implementation_type}, the API access is introduced.
Most of the task based programming models are available as libraries or languages.
During our experiments, we used one of each implementation type.

\begin{table}[H]
	\caption{Implementation Type property for each task based programming model \label{tab:taxo:prop_implementation_type}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Implementation Type_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Implementation Type_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Data Persistence}
This property indicates if the task based programming model supports data persistence.

The support for data persistence is given in Table \ref{tab:taxo:prop_data_persistence}.
The task based programming models predominantly do not support data persistence.
Task based programming models using the file system to transfer data between the tasks can be considered as using data persistence since data stays if it is not deleted by the programming models after errors or applications terminations.

\begin{table}[H]
	\caption{Data Persistence property for each task based programming model \label{tab:taxo:prop_data_persistence}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Data Persistence_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Data Persistence_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Scheduler Location}
This property describes where the scheduler instances of the task based programming models are located.
It can be \textit{centralized} where there only one scheduler instance that manages all the tasks.
The tasks can also be managed at the local level in the workers in a \textit{distributed} way.

In Table \ref{tab:taxo:prop_scheduler_location}, the location of the scheduler is shown for each task based programming model.
Most of the task based programming models use centralized schedulers.
In our experiments, we used task based programming models that use both type of schedulers.

\begin{table}[H]
	\caption{Scheduler Location property for each task based programming model \label{tab:taxo:prop_scheduler_location}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Scheduler Location_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Scheduler Location_p2.tex"}
	\end{multicols}
\end{table}

%\subsubsection{}
%\begin{table}[H]
%	\caption{}
%	\centering
%	\input{"chapters/taxonomy/_table_taxo_.tex"}
%\end{table}


%\subsection{Other ideas}
%\subsubsection{Scheduling policy}
%The scheduling policy is how the tasks are ordered during the execution of the task-based application.
%The dependencies between the tasks are respected by the scheduler.
%It can also be optimized to reduce the data migrations across the nodes and try to execute tasks on the node where the data are present.
%
%\subsubsection{Dynamic workflow}
%It represents the ability of the scheduler to adapt its policy during runtime depending on the results of the tasks.
%For instance, stop the application when the convergence is attained.
%
%
%\subsubsection{Multi-backend}
%Is the programming paradigm able to use several backends ?
%Is it possible to use code in other languages as tasks ?


%\subsection{Learnability}
%\subsection{Understandability}
%\subsection{Communicativeness}

\section{Taxonomy Summary}

\begin{table}[H]
	\caption{Taxonomy I}
	\centering
	\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
	\input{chapters/taxonomy/_table_taxo_cat1.tex}
\end{table}

\begin{table}[H]
	\caption{Taxonomy II}
	\centering
	\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
	\input{chapters/taxonomy/_table_taxo_cat2.tex}
\end{table}

\begin{table}[H]
	\caption{Taxonomy III}
	\centering
	\input{chapters/taxonomy/_table_taxo_cat3.tex}
\end{table}


\section{Language Properties Relevant for Task-Based Programming}

\section{Languages Extension}