\chapter{Taxonomy of Task-Based Programming Models}
\label{chap:taxonomy}

In this chapter, the taxonomy and the properties deduced from the usage of several task based programming models is introduced.
This taxonomy also presents how each property is expressed in the different tasks programming models.

\section{Taxonomy}
In this section, we introduce the taxonomy of the task based programming models that is deduced from the studies and experiments.
We divided the features of the taxonomy into three categories.
\textit{Task Capabilities} is the first category which contains the features related to the abilities of the tasks such as the granularity of the tasks and the architectures they can be executed on.
The second category is \textit{Task and Data Management} which contains the features related to the scheduling properties of the tasks and data migrations.
The last category is \textit{Programming Model Features} in which we present capabilities of the programming model itself such as data persistence and fault tolerance.


\subsection{Task Capabilities}
In this section, the features related to the category \textit{Task Capabilities} are presented.
They include task features that we believe interesting in the choice of a task based programming model to implement the tasks in a task based application.
The \textit{Task Granularity} can help to understand the scale and the amount of computations of the tasks.
The memory \textit{Architecture} is also a metric that help to understand the scale at which the task based programming models can be used to implement an application.
The \textit{Heterogeneity} and the \textit{Portability Accelerator} help to understand if the task can be executed on different accelerators with the same implementation and on which type of accelerators.
These informations are critical depending on the cluster or supercomputers architectures targeted.
The \textit{Task Implementation} represents the interface available to user to implement the tasks and the kind of effort needed to port an existing kernel application into a task.
Finally, the \textit{Data Handling} shows how the data are accessed from the tasks and can help to understand the effort necessary to use and perform operations on the data in the tasks.

\subsubsection{Task Granularity}
This property represents the amount of resources on which a task can be executed.
For this study, we consider that either \textit{sequential}, \textit{parallel} or \textit{parallel and distributed} code can be executed as task.
Usually, a sequential task is run as a lightweight thread, a thread or a single process without multi-threading.
A parallel task can also use multiple threads or processes up to one node like OpenMP or MPI on shared memory.
Finally, distributed and parallel resources like multiple nodes from a cluster can be allocated to parallel and distributed tasks.
These tasks could be able to execute MPI or PGAS based code on their allocated resources.

As seen in Chapter \ref{chap:exp_dense}, a good balance between the number of tasks and the work load given to the task is required to express enough parallelism so that the computing resources can execute tasks and to make sure there is not too much overhead from having too many tasks.
Therefore, sequential tasks are more likely to be fine grain tasks that process a relatively small amount of data while distributed and parallel tasks are more likely to process more data and have a larger grain.
For instance, YML+XMP tasks are distributed and parallel while HPX, PaRSEC and Regent tasks are sequential tasks that run in threads.
In Table \ref{tab:blocks}, we explore the number of blocks used in YML+XMP, HPX, PaRSEC and Regent to get the best results for our task based LU factorization for a 16384 $\times$ 16384 matrix.
The size of the block can be computed by dividing the size of the matrix by the number of blocks.
Therefore, the higher the number of blocks, the smaller the size of the block is.
In this case, there is up to 8 $\times$ 8 blocks for YML+XMP whereas there is at least 30 $\times$ 30 blocks for Regent, HPX and PaRSEC.
Thus, YML+XMP distributed and parallel tasks managed around 16 times ($ \sim (30/8)^2 $) more data than the largest task executed by Regent, HPX and PaRSEC.
Moreover, these tasks are matrix products and matrix inversions which correspond to $\sim n^3$ operations where $n$ is the dimension of the matrix.
So, YML+XMP tasks performed around 64 times ($ \sim (30/8)^3 $) more operations.

In Table \ref{tab:taxo:prop_task_granularity}, the granularity of the tasks of the considered task based programming models is presented.
Most of the task based programming models use sequential tasks.
For our studies and experiments, we used task based programming models for distributed memory with sequential tasks and with parallel and distributed tasks.

\begin{table}[H]
	\caption{Task Granularity property for each task based programming model \label{tab:taxo:prop_task_granularity}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Task Granularity_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Task Granularity_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Architecture}
The architecture supported by the task based programming models is an important property to take into consideration since it will impact the scale at which the task based programming models can be used.
There is two main scale used in high performance computing : the shared memory and the distributed memory.
Shared memory involves the usage of limited resources since these task based programming models can only be used on one cluster or supercomputer since they do not provide means to transfer data between several nodes.
They can be used with data transfer libraries to address distributed memory.
One of such case is the use of OpenMP on the nodes and MPI to manage distributed memory.
They can also be used in a task that could run on a complete node.
On the other hand, with distributed memory, it is possible to address multiple nodes, thus, as much resources as available.
In this case, the programming models that can manage distributed memory are able to migrate data between cluster nodes.
Therefore, the two values considered for this property are the \textit{shared memory} and the \textit{distributed memory} architectures.

Shared memory task based programming models cannot be used to address exascale computing without being used with another technology to manage distributed memory.

In Table \ref{tab:taxo:prop_architecture}, the architecture which is supported by the considered task based programming models is introduced.
Task based programming models that manage shared memory can only execute sequential tasks expect for QUARK that can execute multi-threaded functions.
On the other hand, some of the task based programming models that manage distributed can execute parallel and distributed tasks.

\begin{table}[H]
	\caption{Architecture property for each task based programming model \label{tab:taxo:prop_architecture}}
	\centering
	\begin{multicols}{2}
		\input{chapters/taxonomy/_table_taxo_Architecture_p1.tex}

		\input{chapters/taxonomy/_table_taxo_Architecture_p2.tex}
	\end{multicols}
\end{table}

\subsubsection{Heterogeneity}
This property indicates if the programming model supports accelerators (for instance, GPUs).
\textit{Explicit} support means that the user has to provide the implementation of the task that will be run on the accelerator.
\textit{Implicit} support means that the tasks can be run on different devices while the user has to provide only one implementation.
In this case, the programming models also manage data transfer between the CPUs and the GPUs memory.
For instance, tasks implemented with OpenMP can be run both on CPUs and GPUs while OpenMP manages the data offloads to the GPU.

In Table \ref{tab:taxo:prop_heterogeneity}, it is shown if the accelerator support is explicit or implicit for the considered task based programming models.
Most of the task based programming models do not support a direct implementation of the tasks for accelerators.
Indeed, the users have to provide such implementation if they want to execute tasks addressing accelerator computations.
Our experiments were mostly performed on CPUs so this feature was not used during our experiments.

\begin{table}[H]
	\caption{Heterogeneity property for each task based programming model \label{tab:taxo:prop_heterogeneity}}
	\centering
	\begin{multicols}{2}
		\input{chapters/taxonomy/_table_taxo_Heterogeneity_p1.tex}

		\input{chapters/taxonomy/_table_taxo_Heterogeneity_p2.tex}
	\end{multicols}
\end{table}

\subsubsection{Data Handling}
This property describes how the data are accessed in a task.
The data can be accessed \textit{directly}.
For instance, the data can be accessed through function parameters.
The data can also be requested or retrieved from a \textit{container}.
For instance, the data can be retrieved through a future in HPX.

This represents the current (as experienced during this dissertation) implementations of the task based programming models and can evolve with time.

In Table \ref{tab:taxo:prop_data_handling}, how to access the data from the tasks is described.
Most of the programming models provide a direct access to the input and output data for tasks whereas only a few of them require the user to request for the data they use in their tasks.
During our experiments, we used HPX that uses futures to retrieve data in tasks and YML+XMP that provide a direct access to the data.

\begin{table}[H]
	\caption{Data Handling property for each task based programming model \label{tab:taxo:prop_data_handling}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Data Handling_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Data Handling_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Task Implementation}
This property indicates what kind of interface the user has to fill in to create a task which will be executed during the execution of the application.
The tasks can use a \textit{program} where the user has to provide the parameters through the API such as Pegasus.
It can also use a \textit{function pointer} which the user has to pass to the API as well as its parameters.
The programing paradigm can also be based on \textit{pragmas} which are used to delimit and describe the task.
Another possibility is a custom interface like a \textit{function with specific parameters}.
The tasks can also be implemented by \textit{encapsulating} them such as in YML+XMP where the task code is encapsulated in an XML file.
Moreover, the tasks can be implemented using the dedicated language of the task based programming model.
For instance, Regent task have to be implemented with Regent which allows the conversion of the tasks into CPU and GPU code.

Task encapsulation or function calls with existing softwares is an important feature and useful for task based programming models since it allows to reuse already implemented functions in tasks.
The programming model should be able to provide a way to easily access data from a task to call an existing library e.g. existing code already written in C/C++/Fortran.
This allows faster developments since existing code can be reused.

In Table \ref{tab:taxo:prop_task_implementation}, how the tasks are implemented is detailed.
During our experiments, we used Regent with its dedicated language based on Lua and Terra to implement applications and tasks.
We also used HPX that uses function pointers to implement tasks as well as YML+XMP and PaRSEC.

\begin{table}[H]
	\caption{Task Implementation property for each task based programming model \label{tab:taxo:prop_task_implementation}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Task Implementation_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Task Implementation_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Portability Accelerators}
This property indicates what kind of accelerators are supported by the task based programming models.
They can support or generate \textit{CUDA} code and execute it on NVIDIA GPUs.
There is also the possibility of supporting \textit{multiple} accelerator architectures.
This may be achieved by having a suitable backend for each supported accelerator architecture.
Another possibility is to let the \textit{user} chose how and which type of accelerator to use by interfacing with an accelerator programming language.

It depends on the existing technologies and can evolve over time or with apparition of new technologies.

Furthermore, the multiplication of supercomputer architectures makes the portability between supercomputers difficult especially if the support of multiple architectures is necessary.
Therefore, the portability of an application is mandatory if there is a large amount of users.
Indeed, in a large user base, there is a high probability that the users will have access to different computer architectures.
At least, the architecture of their local computers should be different than the supercomputer they have access to.
Portability to a large set of architectures allows the users to be able to familiarize with the application locally and the developers to easily make local modifications and tests then experiment them on larger scale on supercomputers.

In Table \ref{tab:taxo:prop_portability_accelerators}, the accelerators supported by the different task based programming models are shown.
Our applications do not support accelerators so we were not able to try this feature.
Nvidia cards are the most used GPUs so CUDA that can address Nvidia GPU programming is supported by most of the task programming models that can execute tasks on GPUs.
However, other constructors are producing GPUs such as Intel and AMD.

\begin{table}[H]
	\caption{Portability Accelerators property for each task based programming model \label{tab:taxo:prop_portability_accelerators}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Portability Accelerators_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Portability Accelerators_p2.tex"}
	\end{multicols}
\end{table}

\subsection{Task and Data Management}
In this section, the features related to the category \textit{Task and Data Management} are introduced.
They include task and data scheduling properties that we believe interesting in the choice of a task based programming model to implement a task based application.
The \textit{Dependency Type} is an interesting feature to know which  type of dependency the user has to extract from his/her algorithm and to provide during the implementation of the task based application.
The \textit{Worker Management} indicated the user investment in term of application management.
The \textit{Data Distribution} describes how the scheduler manage the placement of the data on the computing resources.
This placement can be made by the scheduler or directly by the user which demands more effort.
The \textit{Task Binding} represents the if the user has to provide the binding between tasks and computing resources.
The \textit{Task Insertion} indicates if the scheduler is capable of adding new tasks to schedule during the execution of already scheduled tasks.

\subsubsection{Dependency Type}
This property describes how the dependencies between the tasks are provided by the user of the task based programming models.
This means that depending on the provided informations (in task description, in particular), the scheduler can deduce other dependency types.
For instance, the control dependency graph combined with the informations on the use of the data parameters provided to the task could allow the scheduler to deduce the data dependency graph.
Therefore, the user has to provide the required informations so that the scheduler is able to work properly.

The user may have to provide \textit{control} dependencies between the tasks.
They describe in which order the tasks can be executed and which tasks can be executed at the time.
\textit{Data} dependencies can also be provided.
This approach is used to infer the dependencies and the parallelism between the tasks by studying how the data are used by the tasks and how they flow from one task to the other.
The user may have to provide \textit{both} dependency type although it is possible to convert a dependency graph type into the other with the appropriate informations.

In Table \ref{tab:taxo:prop_dependency_type}, the dependency type used to describe the dependencies between the tasks in task based programming models is introduced.
In our experiments, we used data oriented such as HPX and control oriented dependency programming models such as YML+XMP.

\begin{table}[H]
	\caption{Dependency Type property for each task based programming model \label{tab:taxo:prop_dependency_type}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Dependency Type_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Dependency Type_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Worker Management}
This property indicates whether the worker thread or process which hosts the tasks in the task based programming models has to be started and maintained by the user (\textit{explicit}) or is provided by the runtime (\textit{implicit}).
This means the user has to put more efforts to launch its application in the explicit case.

In Table \ref{tab:taxo:prop_worker_management}, the worker management is shown for each task based programming model.
Most of the task based programming models manage their worker placement to execute the tasks without user interference.
Only a few need user intervention while some of them allow hints from the user.

\begin{table}[H]
	\caption{Worker Management property for each task based programming model \label{tab:taxo:prop_worker_management}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Worker Management_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Worker Management_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Data Distribution}
This property describes how the data distribution is handled by the programming model scheduler.
The scheduler can be able to migrate data between computing resources.
\textit{Implicit} data distribution means that the runtime system decides where to place the data on the nodes whereas \textit{explicit} data distribution means that the user has to specify the distribution of the data across the nodes.
In the implicit case, the scheduler has more liberty to optimize the computations and the data migrations when the scheduler is able to do so.

When the schedulers will be more efficient and perform better, the implicit management of the data will be very interesting to manage data.
However, it is still not the case as we showed in Chapter \ref{chap:exp_km}.

In Table \ref{tab:taxo:prop_data_distribution}, the data distribution management in the task based programming models is introduced.
In our YML+XMP applications, we had implicit data distributions for the tasks but not in the parallel and distributed tasks in which we had to place our data.
For our HPX and PaRSEC applications, we had to explicitly map our data with process ranks representing cores or nodes from a cluster or supercomputer.

\begin{table}[H]
	\caption{Data Distribution property for each task based programming model \label{tab:taxo:prop_data_distribution}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Data Distribution_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Data Distribution_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Task Binding}
This property describes how the tasks are bound to the allocated hardware resources.
It demands more effort to the scheduler in order to place the tasks on the resources but with efficient enough schedulers, it will allow to increase the performances of the task based programming models.
The binding can be \textit{implicit} when automatically determined by the task based programming runtime or \textit{explicit} when the user has to provide a binding map.

In Table \ref{tab:taxo:prop_task_binding}, the task binding management of the task based programming models is highlighted.
All the programming models propose a implicit task binding with some of them allowing the user to pin tasks on computing resources.
For our experiments we did not try to optimize the task binding by giving hints to PaRSEC and HPX that support explicit bindings.

\begin{table}[H]
	\caption{Task Binding property for each task based programming model \label{tab:taxo:prop_task_binding}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Task Binding_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Task Binding_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Task Insertion}
This property indicates if new tasks can be added to the task pool during the execution of the already scheduled tasks.
This feature was not required for the studied applications but it is an important feature for certain applications such as iterative methods.
This feature can allow them to start a new iteration or stop iterating according to a given metrics that can be the output of a task.
For instance, HPX provides such features.

In Table \ref{tab:taxo:prop_task_insertion}, the insertion of new tasks during the execution of the tasks is described.
This feature is very useful for iterative methods but we did not implement such methods so we did not use this feature in our experiments.

\begin{table}[H]
	\caption{Task Insertion property for each task based programming model \label{tab:taxo:prop_task_insertion}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Task Insertion_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Task Insertion_p2.tex"}
	\end{multicols}
\end{table}

\subsection{Programming Model Features}
In this section, the features related to the category \textit{Programming Model Features} are described.
They include programming model properties that we believe interesting in the choice of a task based programming model to implement a task based application.
The \textit{Dependency Expression} property describes the type of structures that hold the dependencies between the tasks and can help the user understand how he/she has to express the dependencies during the implementation.
The \textit{Communication Model} property can help the user chose the communication model the most appropriate for the application and the cluster or supercomputer on which the application will be executed.
The \textit{Fault Tolerance} property indicates if the programming model supports fault tolerance.
The \textit{Implementation Type} property refers to how the programming model is implemented and can help the user to understand how the programming model will be integrated in the development and the deployment of the application.
The \textit{Data Persistence} property indicates if the task based programming model support data persistence which can be a mandatory feature needed for an application.
Finally, the \textit{Scheduler Location} property helps the user to understand the scheduling policy of the task based programming model schedulers.

\subsubsection{Dependency Expression}
This property describes how the dependencies between the tasks are represented in the considered model.
The possibilities include a \textit{graph}, a \textit{directed acyclic graph} or DAG, a \textit{tree} and a \textit{Petri Net}.

In Table \ref{tab:taxo:prop_dependency_expression}, how the dependency are expressed in each task based programming model is detailed.
The difference between graph and DAG is not always clear in the implementations of the task based programming models.
In this table, we recorded the informations we found in papers introducing the task based programming models.

\begin{table}[H]
	\caption{Dependency Expression property for each task based programming model \label{tab:taxo:prop_dependency_expression}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Dependency Expression_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Dependency Expression_p2.tex"}
	\end{multicols}
\end{table}


\subsubsection{Communication Model}
This property describes how data are sent from a task to another.
The runtime system can use \textit{message passing} (msg), \textit{global address space} (gas) or the \textit{file system} (fs).

This feature can provide many interesting optimizations such as data migration anticipation or data pre-loading.
However, these features are not implemented yet.
They also induces complex choices for the scheduler that may not improve performances at the end.

In Table \ref{tab:taxo:prop_communication_model}, the communication model used in each task based programming model is shown.
Task based programming models that only work in shared memory do not have a communication model since they do not manage distributed memory.
In our applications, we used programming models that use each communication model.

\begin{table}[H]
	\caption{Communication Model property for each task based programming model \label{tab:taxo:prop_communication_model}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Communication Model_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Communication Model_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Fault Tolerance}
This property indicates if the task based programming models support fault tolerance.
Fault tolerance allows applications to recover from errors during execution.
If errors still appear, the application can be stopped properly.

In Table \ref{tab:taxo:prop_fault_tolerance}, the fault tolerance support is given for each task based programming model.
Task based programming models that support fault tolerance are mainly the ones that can execute distributed and parallel tasks such as YML+XMP we used to implement our applications.

\begin{table}[H]
	\caption{Fault Tolerance property for each task based programming model \label{tab:taxo:prop_fault_tolerance}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Fault Tolerance_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Fault Tolerance_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Implementation Type}
This property describes how the programming model API is included in an application.
It can be done through a \textit{library}, a \textit{language extension} or a \textit{language}.
This property shows how the programming models are used to implement an application with them.

In Table \ref{tab:taxo:prop_implementation_type}, the API access is introduced.
Most of the task based programming models are available as libraries or languages.
During our experiments, we used one of each implementation type.

\begin{table}[H]
	\caption{Implementation Type property for each task based programming model \label{tab:taxo:prop_implementation_type}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Implementation Type_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Implementation Type_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Data Persistence}
This property indicates if the task based programming model supports data persistence.

The support for data persistence is given in Table \ref{tab:taxo:prop_data_persistence}.
The task based programming models predominantly do not support data persistence.
Task based programming models using the file system to transfer data between the tasks can be considered as using data persistence since data stays if it is not deleted by the programming models after errors or applications terminations.

\begin{table}[H]
	\caption{Data Persistence property for each task based programming model \label{tab:taxo:prop_data_persistence}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Data Persistence_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Data Persistence_p2.tex"}
	\end{multicols}
\end{table}

\subsubsection{Scheduler Location}
This property describes where the scheduler instances of the task based programming models are located.
It can be \textit{centralized} where there only one scheduler instance that manages all the tasks.
The tasks can also be managed at the local level in the workers in a \textit{distributed} way.

In Table \ref{tab:taxo:prop_scheduler_location}, the location of the scheduler is shown for each task based programming model.
Most of the task based programming models use centralized schedulers.
In our experiments, we used task based programming models that use both type of schedulers.

\begin{table}[H]
	\caption{Scheduler Location property for each task based programming model \label{tab:taxo:prop_scheduler_location}}
	\centering
	\begin{multicols}{2}
		\input{"chapters/taxonomy/_table_taxo_Scheduler Location_p1.tex"}

		\input{"chapters/taxonomy/_table_taxo_Scheduler Location_p2.tex"}
	\end{multicols}
\end{table}

%\subsubsection{}
%\begin{table}[H]
%	\caption{}
%	\centering
%	\input{"chapters/taxonomy/_table_taxo_.tex"}
%\end{table}


%\subsection{Other ideas}
%\subsubsection{Scheduling policy}
%The scheduling policy is how the tasks are ordered during the execution of the task-based application.
%The dependencies between the tasks are respected by the scheduler.
%It can also be optimized to reduce the data migrations across the nodes and try to execute tasks on the node where the data are present.
%
%\subsubsection{Dynamic workflow}
%It represents the ability of the scheduler to adapt its policy during runtime depending on the results of the tasks.
%For instance, stop the application when the convergence is attained.
%
%
%\subsubsection{Multi-backend}
%Is the programming paradigm able to use several backends ?
%Is it possible to use code in other languages as tasks ?


%\subsection{Learnability}
%\subsection{Understandability}
%\subsection{Communicativeness}

\section{Taxonomy Summary}
In this section, we present a summary for the features presented in each category as well as the expression of these features so that it is easier to compare the features of the different task based programming models.

Table \ref{tab:taxo:sum_cat2} summarizes the \textit{Task Capabilities} category.
In this category, we introduced the \textit{Task Granularity} which can take the values Sequential tasks (s) as well as parallel and distributed tasks (p).
We also indicated if the programming models support \textit{Nested Tasks}.
The values for the property \textit{Task Implementation} are Dedicated Language (dl), Function Pointer (fp), Program (pgm), Pragma (p) and Code Encapsulation (ce).
We detailed if the \textit{Heterogeneity} is implicit (i) or explicit (e).
We explained the memory \textit{Architecture} values which are Distributed Memory (d) and Shared Memory (s).
We introduced the \textit{Data Handling} that can be a direct access (d) or made through a request to the data management system (c).
Withe the \textit{Portability Accelerators} property, we gave the technology that can be used to address accelerators with the task based programming models.

Table \ref{tab:taxo:sum_cat1} summarizes the \textit{Task and Data Management} category.
We gave the \textit{Dependency Type} used to express the dependencies between the tasks which are data (d), control (c) or both (b).
We also indicated if the task based programming models support \textit{Task Insertion}.
We described \textit{Data Distribution}, \textit{Worker Management} and \textit{Task Binding} properties which values are implicit (i) or explicit (e).

Table \ref{tab:taxo:sum_cat3} summarizes the \textit{Programming Model Features} category.
We indicated if the task based programming models support \textit{Fault Tolerance} and \textit{Data Persistence}.
We explained which structure holds the dependencies between the tasks in \textit{Dependency Expression}.
We detailed which \textit{Communication Model} is used.
The values can be Global Address Space (gas), Shared Memory (sm), File System (fs) and Message Passing (msg).
We also introduced the \textit{Implementation Type} of the task based programming models which can be a language (lang), a runtime system (rt), a library (lib) and a language extension (ext).
We gave the \textit{Scheduler Location} which can be external (e), distributed (d) or centralized (c).

\begin{table}[H]
	\caption{Task Capabilities Summary \label{tab:taxo:sum_cat2}}
	\centering
	\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
	\input{chapters/taxonomy/_table_taxo_cat2.tex}
\end{table}

\begin{table}[H]
	\caption{Task and Data Management Summary \label{tab:taxo:sum_cat1}}
	\centering
	\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
	\input{chapters/taxonomy/_table_taxo_cat1.tex}
\end{table}

\begin{table}[H]
	\caption{Programming Model Features Summary \label{tab:taxo:sum_cat3}}
	\centering
	\input{chapters/taxonomy/_table_taxo_cat3.tex}
\end{table}

We summarized the different features and values we introduced in this taxonomy.
Now, we can propose recommendations for implementing task based applications and improving task based programming models.

\section{Analyze and Recommendations}
In this section, we will provide recommendations for implementing task based applications and improving task based programming models.
We introduce them by importance order.
We start with those we believe are the more important.

\subsection{Adapted Programming Model to Algorithm Granularity}
Choosing the right granularity for tasks is an important parameter to obtain optimal performances with task based programming models.
Moreover, the number of tasks is also important to get optimal performances since too much tasks induces a large scheduling and task launching overhead whereas not enough tasks does not expose enough parallelism nor computations to take advantage of all the computing resources available which is sub-optimal.
Thus, an compromise between the number of tasks executed and the granularity of the tasks is mandatory as we experienced on the K Computer in Chapter \ref{chap:exp_dense}.
Besides, the granularity also depends on the cluster or supercomputer used to execute the applications due to the difference of core performances.
Thus, the compromise has to be found for each different architecture.

\subsection{Data Migrations}
As data migrations are costing more time and energy due to the larger networks, it is mandatory to favor programming models optimizing them in order to reduce their usage while still obtaining good performances.
In order to do so, tasks based programming models require the data and control dependencies between the tasks to be able to optimize the computations placement on the computing resources as well as the data migrations.
Data migrations comprise communications between nodes, data copy between caches, offloading to accelerators and IOs with the file system.
An efficient scheduler could favor data locality and cache reuse depending on the granularity of the tasks.


\subsection{Encapsulated Tasks}
As we try to reduce the communications between the nodes of the clusters and supercomputers in order to improve performances, it is not advised to make let the tasks make communications during their executions.
The only data transfer that should happen in tasks are during their initialization with their input parameters and during their finalization with the output parameters.
Note that parameters could be inputs and outputs.
In this case, the are modified by the tasks and the programming model should be able to manage such cases.
This way, the communications and the parameters of the tasks can be managed efficiently by the task based programming model schedulers.

\subsection{Dependencies Expression}
The dependencies should be expressed as a graph or an equivalent.
It is not mandatory to explicitly be a graph.
However, under the format of a graph, it may be possible to use graph exploration methods in order to optimize the data distribution and the computations placement so that the communications are minimal and the maximum of computations can run at the same time.
For instance, YML multidimensional arrays of event used to represent dependencies can be transformed into graphs then used to optimize execution of the tasks and the data migrations.
With graphs, even irregular dependencies can be expressed which may no be the case with other dependencies representations.

\subsection{Dynamical Task Scheduling}
With dynamical scheduling, it is possible to introduce conditional branchings in the graph of dependencies.
For instance, this could be used to stop iterative methods expressed with graphs of tasks when they reach their stopping conditions.
Dynamical graphs for tasks execution in YML+XMP were introduced in \cite{Wu2019}.


\subsection{High Level Languages}
Although pragmas are a fast solution to implement task based applications, in practice, only task based programming models working on shared memory choose this approach such as OpenMP which is based on directives and OmPSs which is an extension of OpenMP.
Higher level programming models are suitable to express complex dependencies and give task related informations.
For instance, Kokkos is a high level library that also provides a tasking interface.

\subsection{Fault Tolerance}
With the increase of the usage of computing resources for the execution of an application, the probability of a failure appearing has increased.
Indeed, even if the probability of a failure on a given computing resource is low, the large amount of computing resources used means that the probability of a failure occurring on one of the computing resource is higher.
Therefore, fault tolerance is an important feature to manage the increasing probability of a failure occurring.
Moreover, fault tolerance can be integrated in schedulers since most of the failures should happen during tasks execution.

If a task fails, it can be re-scheduled and executed on another resource.
If it still fails, it is probably an error in the task then the application should stop.
If tasks are regularly failing on a computing resource and are working fine on other computing resources, it means that there is a problem with this computing resource and it should be excluded from the list of available resources.

In case of missing data, the scheduler could re-execute previously executed task in order to rebuild the missing data from the data still available to the scheduler or stop the application if the missing data cannot be recovered.
Check-pointing can be a solution to save critical data regularly and rebuild missing data if necessary.

\subsection{Check-Pointing}
There is only few programming models that provide check-pointing.
Usually, it is done via external and dedicated tools.
A scheduler could be implemented to regularly save the output of tasks.
Then the scheduler could re-start the application and continue executing task from the last saved task parameters.
In conjunction with fault tolerance, check-pointing could allow the recovery of data lost during computing resources failures and the success of the application even with critical resource failures.

\subsection{Multi-Level Programming}
Multi-level programming can be an interesting solution to express very efficient applications.
Indeed, it could be possible to express large grain tasks with high level task based programming models and use parallel and distributed tasks.
Then these tasks could be implemented with lower level programming models that are very efficient with medium sized applications.
For instance, YML+XMP+StarPU is such an example.
YML was used to express large grain task and dependencies.
Parallel and distributed tasks were implemented with XMP and small grain computations at the level of the processes were managed by StarPU.
Another possibility is to implement large grain tasks as a smaller grain task based application.

\subsection{Collective Operations}
Collective operations are the equivalent to collective communications defined in MPI such as all-to-all, one-to-all or all-to-one communications for task based applications.
Collective communications include reductions, gathers and scatters.
They exist in an one-to-all version as well as an all-to-all version depending on where the results are expected to be sent.
Such operations on task outputs do not exist natively in most of the task based programming models.
Therefore, efficiently implementing such task operations can help to obtain better performances when these operations are needed, for instance, in the task based sparse matrix vector product implemented in Chapter \ref{chap:exp_sparse}.

A possible implementation for a reduction could be implemented as high level operation that takes an array of task output dependencies and a reduction operation that will be performed on the data such as a sum, a multiplication or a maximum.
The reduction operation has to be provided by the user for custom types defined by the user.
It corresponds to the task performed on two data to reduce it.
It also should return the result of the reduction operation.
Then, the results can be reused as input for other tasks.
This high level reduction could create a dependency graph to efficiently perform the reduction operation with every data dependency provided.
Afterwards, the scheduler can schedule and execute tasks according to the generated dependencies and the provided reduction operation.

Furthermore, other collective operations could be implemented following this process.
They could be reused efficiently multiple times in an application.
Moreover, they could be efficiently implemented and even improved when possible.

\section{Synthesis and Perspectives}
In this chapter, we introduced the features that we believe interesting for task based programming models and how they are expressed in the different task based programming models studied in this dissertation.
We divided the features into three categories : \textit{Task Capabilities} which contains the features related to the abilities of the tasks, the \textit{Task and Data Management} which contains the features related to the scheduling properties of the tasks and data migrations as well as \textit{Programming Model Features} in which we present capabilities of the programming models.
Then, we provided recommendations to improve the task based programming models according to the features they possess and the features we believe task based programming should have in order to increase their capabilities and performances.

To go further, more features and their values for each task based programming models could be added in the taxonomy.
Furthermore, the recommendations could be integrated into some of the existing task based programming models and make experiments with the additions to show their efficiency.
Another possibility is to implement a new task based programming model showcasing the recommendations as main features.
