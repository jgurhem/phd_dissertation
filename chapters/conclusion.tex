\chapter{Conclusion and Perspectives \label{chap:ccl}}
\section{Synthesis}

% chap 2
First, we introduced the High Performance Computing (HPC) time line as well as most of the task based programming models used by the HPC community.
We also discussed the upcoming challenges for exascale in HPC.

% chap 3
Afterwards, we explained the three kind of methods we implemented with several programming models in this dissertation.
The first algorithms we introduced are block-based direct methods tp solve dense linear systems; the block Gaussian elimination, the block Gauss-Jordan elimination and the LU factorization with the forward and backward substitutions to solve the two triangular systems.
The second algorithm is the sparse matrix vector product.
In this part, we introduced several sparse matrix storage formats that are more efficient to store matrices with large amount of zeros in memory.
In particular, we explained the compressed sparse row (CSR), Ellpack (ELL) and coordinates (COO) sparse storage formats as well as their corresponding algorithms to perform the sparse matrix vector product.
The last method is the Kirchhoff seismic pre-stack depth migration which is commonly used in geoscience and by Total to study the underground.

% chap 4
Thereafter, we described several task based programming models in greater details as our first contribution.
In this study, we are interested in task based programming models working on distributed memory.
we split them into two categories : fine grain task-based distributed and parallel programming models in which tasks are executed on one thread or process and task-based distributed and parallel programming models in which tasks are executed on multiple processes or nodes.
For the first category, we selected Regent, Legion, HPX, PaRSEC and TensorFlow.
For the second category, we selected YML+XMP, Pegasus and Swift.
We highlighted the main features of each programming models studied and how to use them to define task, the dependencies between them, how to register the data used in the tasks and how to execute the tasks to perform the intended computations.

% chap 5
Then, we introduced our task-based applications to solve dense linear systems and perform the LU factorization.
These applications are part of our second contribution.
We introduced three block based direct methods to solve dense linear systems : the block Gaussian elimination, the block Gauss-Jordan elimination and the LU factorization.
We implemented the solution to dense linear systems in YML+XMP, XMP, MPI and ScaLAPACK whereas the LU factorization application was also implemented with HPX, Regent and PaRSEC.
We performed experiments on the K Computer with YML+XMP and XMP.
We showed that the YML+XMP applications were running faster than the XMP corresponding applications on 8096 cores.
We also performed experiments on Poincare, \textit{La Maison de la Simulation} cluster, with the LU factorization implemented with the task based programming models YML+XMP, HPX, PaRSEC and Regent as well as MPI, XMP and ScaLAPACK.
We performed strong scaling experiments with each application up to 64 nodes for $16384 \times 16384$, $32768 \times 32768$ and $49512 \times 49512$ matrices.
We showed that MPI has the fastest execution times and the best scalability on 64 nodes and that, although, XMP translates its directives into MPI code, the PGAS model used in XMP is not as efficient as using directly MPI whereas HPX is the most efficient task based programming model on 64 nodes.
However, PaRSEC also shows interesting performances in some cases and Regent applications execution time increase while increasing the number of nodes from 32 to 64 which should not be the case.
We showed that HPX and PaRSEC are performing better with a higher number of smaller tasks.
YML+XMP execution times are higher than the other task based programming models due to the use of the file system but its strong scaling speedups are higher so YML+XMP could obtain better performances on a larger number of resources as we showed on the K Computer.

% chap 6
Subsequently, we described our sparse matrix vector product based application with its 2D block decomposition used to store our matrices in which each block is a sparse matrix compressed with a sparse storage format.
This application is also included in our second contribution.
We analyzed, evaluated and experimented the sparse operation $A(Ax+x)$ using HPX, YML+XMP and MPI.
The applications are implemented on top of a common kernel that performs the sparse matrix vector multiplication on matrices stored in CSR, ELL, COO and SCOO sparse classical storage formats.
We showed that the CSR sparse format storage obtains the best performances in most of the cases and that the COO storage format has better load balancing but requires larger output vector which increases the size of collective operations, therefore their execution time.
We have shown that collective operations are an issue in task-based programming models since most of them do not provide a highly optimized implementation and generate all-all communications.
We also have shown that collective communications in MPI are taking more and more time as the number of computing resources used increases, especially for weak scaling experiments.
Therefore, task-based programming models, that can avoid large scale collective communications by turning them in collective operations that can be efficiently scheduled or by running them on a subset of the allocated resources, is an interesting alternative to MPI.
However, current systems are more suitable for message passing libraries since both are designed and optimized to work well together.
Moreover, file systems are not suitable either for programming models that use IOs to transfer data between tasks.
The current task-based programming model schedulers lack the capability to completely manage the memory it uses and are not efficient enough to reduce the data migrations.

% chap 7
Thereupon, we introduced task based algorithms for the Kirchhoff seismic pre-stack depth migration and the parallelism intrinsic to the method.
Then, we presented and implemented a simplified 2D version of the Kirchhoff seismic pre-stack depth migration with HPX, MPI and MPI+OpenMP.
This application is the last part of our second contribution.
We used these applications to perform strong scaling and weak scaling experiments as well as study the influence of the number of OpenMP threads on our kernel.
We showed that our HPX application does not scale very well both in term of weak and strong scaling compared to our MPI application.
We deduced that there were migrations of the images and the traces across the nodes which reduced the performances.
We also showed that our addition of OpenMP directives in the kernel did not bring as good performances as our pure MPI application.

% chap 8
Finally, we summarized the key features that we were able to extract from the experiments and studies performed.
We also showed how they are expressed in the different task based programming models.
Then we used these data to form a taxonomy of the task based programming models.
We also provided recommendations to improve the efficiency and performances of task based programming models and their usage.
Finally, we presented a methodology for parallel and distributed programming based on graphs of tasks for post-petascale supercomputers.
These are our third contribution.

\section{Conclusion}
The purpose of this dissertation was to study and experiment with task based programming models.
We introduced and explained the usage of several task based programming models.
We also implemented several applications to solve dense linear system, an application to perform the sparse operation $A(Ax+x)$ based on a common kernel that performs the sparse matrix vector product with several sparse storage formats and a scientific application used by Total to explore the sub-surface, the Kirchhoff seismic pre-stack depth migration.
Then, we performed experiments with our applications such as strong scaling and weak scaling experiments.
We were able to observe the capabilities and performances as well as the shortcomings and limitations of the task based programming models.
We also showed the behavior of these algorithms while implemented with task based programming models.
The goal was not to make the most efficient parallelization of the algorithms.

We had to explore a large amount of parameters including seven different applications (three direct methods to solve linear systems, the LU factorization alone, the sparse matrix vector product with several sparse storage formats and the Kirchhoff seismic pre-stack depth migration) implemented with multiple programming models such as MPI, XMP and several task based programming models.
Each application had parameters to find to obtain optimal parameters especially the number of tasks and the size of the task for the dense linear applications as well as the data distribution for the sparse applications.
We made as much as possible optimizations to obtain the best performances we could.

We showed that task based programming models are very suitable to solve problems with irregular dependencies such as block dense linear algebra.
However, to achieve good performances, the user has to find the optimal parameters in term of the number and the granularity of tasks in order to balance overhead from task scheduling and execution with enough parallelism and tasks to fully use the available resources.
As for algorithm more based on a fork-join process in which computations are performed on distributed data then the output of the computations has to be combined in order to be reused in a similar fashion, the performances heavily depend on the ability of the programming model to perform the combination of the results.
This is illustrated by our implementation of the sparse operation $A(Ax+x)$ which needs to combine the output of the first sparse matrix vector through a reduction and/or a gather depending on the decomposition of the sparse matrix.
We showed that task based programming models do not provide efficient collective operations that can be executed on multiple data coming from tasks.
That makes algorithm based on fork-join parallelization not suitable for task based programming models without collective operations on a given amount of task outputs.
Lastly, the current task based programming models support interactions with the file system only by directly implementing IOs in tasks.
This means that the scheduler is not aware that the task performs IOs and cannot schedule it accordingly.
Therefore, we were not able to make IOs task properly scheduled by the programming model for our Kirchhoff seismic pre-stack depth migration that heavily relies on the file system to load the input data and the Green functions used to approximate the travel time of the recorded sound waves.
Thus, we generated most of our data during the execution which was not as close to the original application as we would like.
Moreover, we also greatly simplified the application which resulted in less dependencies and a more regular application especially with the implementation of a 2D case instead of the 3D case used in practice.
We implemented the Kirchhoff seismic pre-stack depth migration application from scratch which resulted in an application with less features implemented.
It may have been more interesting to try to port an original and already implemented application to a task based application in order to have more realistic tasks and dependencies.

Besides, we highlighted that task based programming models have great potential in efficiently scheduling data migrations including file system IOs.
The scheduler could also manage fault tolerance due to the possibility of restarting failed tasks on other resources if necessary.
Moreover, the expression of tasks and data dependencies provides the necessary informations to implement check-pointing.
Indeed, the task output and the execution informations such as the position in the execution graph could be regularly and efficiently saved during execution in order to let the scheduler relaunch the application at a previous save depending on the available data.

\section{Future Research}

This work could be completed with applications implemented with other task based programming models such as X10, Chapel, Uintah or Charm++ in order to explore their capabilities to solve such problem.
These experiments could also be reproduced on other cluster or supercomputers.
Our task based Kirchhoff seismic pre-stack depth migration could be improved to be closer to what is done in practice as well as include IOs tasks to test the ability of the task based programming models to schedule such tasks.

Improving the YML+XMP dense matrix type can also be considered since it would allow more flexibility for YML+XMP dense linear algebra applications and allow the use of parameter values that could lead to better results.

A possible amelioration to task based programming models is to improve the capabilities of the schedulers in terms of memory management and task scheduling in order to avoid the data migrations and fit data in the available memory of the nodes.
Implementing efficient task based collective operations will also help to improve performances of task based programming models.

Fault tolerance can also be integrated in task based programming models in order to restart failed tasks.
The scheduler could try to determine if the failure comes from the task implementation by trying to restart the failed tasks on other resources or if the failure comes from the hardware by checking if the workers that execute tasks are still functioning.

Check-pointing is also an interesting feature that could be integrated into task based programming models.
The scheduler could add tasks to save data regularly in oder to restart the application on a given snapshot.